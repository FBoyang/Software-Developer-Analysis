import sys
from importlib import reload
import time
from bs4 import BeautifulSoup
import random
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from urllib.request import Request, urlopen
reload(sys)



driver = webdriver.Chrome() #(executable_path=r"C:\Users\fbyan\Miniconda3\Scripts\chromedriver.exe")

#url2 = 'https://www.linkedin.com/jobs/search/?keywords=Software%20Developer&location=Worldwide&locationId=OTHERS.worldwide'
url ='https://www.glassdoor.com/Job/software-developer-jobs-SRCH_KO0,18.htm'
driver.get(url)
time.sleep(2)
infor = 0

#create a new file in the current directory called job.txt
f = open('job.txt', 'w')
#find content in the first page
html = driver.page_source
soup = BeautifulSoup(html,'html.parser') #lxml
for text in soup.find_all('div',{"class":"jobDescriptionContent desc"}):
    infor += 1
    f.write(text.text)

#deal with the pop-up window in the second page
driver.find_element_by_link_text(str(2)).click()
time.sleep(5)
driver.find_element_by_xpath("//button[@type='button' and @class='mfp-close']").click()
time.sleep(5)
html = driver.page_source
soup = BeautifulSoup(html,'html.parser') #lxml
for text in soup.find_all('div',{"class":"jobDescriptionContent desc"}):
            infor += 1
            f.write(text.text)

def execute_times(times):
    global infor, f
    for i in range(3, times + 1):
        driver.find_element_by_link_text(str(i)).click()
        #driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)
        
        #record the number of job discreption into infor (further we will need to record more metadata, so later we will apply numpy
        #but first check wether the number is correct)
        html = driver.page_source
        soup = BeautifulSoup(html,'html.parser') #lxml
        for text in soup.find_all('div',{"class":"jobDescriptionContent desc"}):
            infor += 1
            f.write(text.text)
            
execute_times(5)

f.write("data size is: " + str(infor))
f.close()
